{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Weak Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "logdir = pathlib.Path('./logs/segment')\n",
    "i = 1\n",
    "while (logdir/f'run{i}').exists():\n",
    "    i += 1\n",
    "logdir = logdir/f'run{i}'\n",
    "logdir.mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "print(f'Logging to: {logdir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# hparams_file = ''\n",
    "hparams_file = './hparams_seg.yaml'\n",
    "\n",
    "if hparams_file:\n",
    "    with open(hparams_file) as f:\n",
    "        hparams = yaml.safe_load(f)\n",
    "else:\n",
    "    hparams = {\n",
    "        'image_size': [224, 224],\n",
    "        'batch_size': 32,\n",
    "        'num_epochs': 10,\n",
    "        'lr': 1e-4,\n",
    "        # model hparams\n",
    "        'recon': 1,\n",
    "        'mask_reg': 1e-10,\n",
    "        'cls_guide': 1e-4,\n",
    "    }\n",
    "\n",
    "writer.add_text('hparams', yaml.dump(hparams, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        neg_dir = os.path.join(root, 'neg')\n",
    "        pos_dir = os.path.join(root, 'pos')\n",
    "\n",
    "        with os.scandir(neg_dir) as it:\n",
    "            neg_files = [entry.path for entry in it if entry.is_file()]\n",
    "        with os.scandir(pos_dir) as it:\n",
    "            pos_files = [entry.path for entry in it if entry.is_file()]\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.pos_files = pos_files\n",
    "        self.neg_files = neg_files\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pos_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with Image.open(self.pos_files[idx]) as img:\n",
    "            pos_img = img.copy()\n",
    "        with Image.open(self.neg_files[idx]) as img:\n",
    "            neg_img = img.copy()\n",
    "\n",
    "        if self.transforms:\n",
    "            pos_img = self.transforms(pos_img)\n",
    "            neg_img = self.transforms(neg_img)\n",
    "\n",
    "        return (pos_img, neg_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = hparams['image_size']\n",
    "batch_size = hparams['batch_size']\n",
    "\n",
    "transforms_list = [\n",
    "    v2.ToImage(),\n",
    "    # v2.RandomHorizontalFlip(),\n",
    "    v2.Resize(image_size),\n",
    "    v2.ToDtype(torch.float, scale=True),\n",
    "]\n",
    "transforms_composed = v2.Compose(transforms_list)\n",
    "\n",
    "dataset = CustomDataset('./dataset/preprocessed/', transforms=transforms_composed)\n",
    "dataset_train, dataset_val = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(train_loader))\n",
    "pos = samples[0][:4]\n",
    "neg = samples[1][:4]\n",
    "print(pos.shape)\n",
    "print(neg.shape)\n",
    "grid_img = torchvision.utils.make_grid(torch.cat((pos, neg), dim=0), nrow=4)\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cls_model\n",
    "cls_model = cls_model.Classifier(num_classes=1).to(device)\n",
    "cls_model.load_state_dict(torch.load('./logs/cls/run1/best_model.pth'))\n",
    "\n",
    "import seg_model\n",
    "model = seg_model.GenWeakSegNet(cls_model, num_classes=2).to(device)\n",
    "\n",
    "from torchinfo import summary\n",
    "print(summary(model, input_size=(batch_size, 3, *image_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    def __init__(self, model, writer, device, batch_size=64):\n",
    "        self.model = model\n",
    "        self.writer = writer\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.mask_colors = [\n",
    "            (0, 0, 1),\n",
    "            (0, 0, 0),\n",
    "        ]\n",
    "    \n",
    "    def vis_samples(self, samples, step, tag):\n",
    "        x_all = []\n",
    "        x_hat_all = []\n",
    "        y_mask_all = []\n",
    "\n",
    "        training = self.model.training\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(samples), self.batch_size):\n",
    "                x = torch.stack(samples[i:i+self.batch_size]).to(self.device)\n",
    "                y_img, y_mask = self.model(x)\n",
    "\n",
    "                # reparameterization of categorical distribution\n",
    "                y_mask1 = y_mask.unsqueeze(2)\n",
    "                sm = F.gumbel_softmax(y_mask1.view(-1, model.num_classes), hard=True)\n",
    "                sm = sm.view(y_mask1.shape)\n",
    "                # compose the image from pixelets and masks\n",
    "                x_hat = torch.sum(sm * y_img, dim=1)\n",
    "\n",
    "                x_all += [x]\n",
    "                x_hat_all += [x_hat]\n",
    "                y_mask_all += [torch.argmax(y_mask, dim=1)]\n",
    "        \n",
    "        self.model.train(training)\n",
    "\n",
    "        x_all = torch.cat(x_all, dim=0)\n",
    "        x_hat_all = torch.cat(x_hat_all, dim=0)\n",
    "        y_mask_all = torch.cat(y_mask_all, dim=0)\n",
    "\n",
    "        y_mask_all1 = y_mask_all.unsqueeze(1)\n",
    "        mc_all = torch.zeros_like(x_hat_all)\n",
    "\n",
    "        for i, color in enumerate(self.mask_colors):\n",
    "            mask = (y_mask_all1 == i).float()\n",
    "            # mask = (torch.randn(y_mask_all1.shape, device=self.device) > 0).float()\n",
    "            color = torch.tensor(color, dtype=torch.float, device=self.device)\n",
    "            color = color.view(1, 3, 1, 1)\n",
    "            mc_all += mask * color\n",
    "        \n",
    "        writer.add_images(f'{tag}/x', x_all, step)\n",
    "        writer.add_images(f'{tag}/x_hat', x_hat_all, step)\n",
    "        writer.add_images(f'{tag}/y_mask', mc_all, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = Visualizer(model, writer, device, batch_size=batch_size)\n",
    "n_vis = 50\n",
    "\n",
    "vx_train = (\n",
    "    [dataset_train[i][0] for i in range(n_vis)] +\n",
    "    [dataset_train[i][1] for i in range(n_vis)]\n",
    ")\n",
    "vx_val = (\n",
    "    [dataset_val[i][0] for i in range(n_vis)] +\n",
    "    [dataset_val[i][1] for i in range(n_vis)]\n",
    ")\n",
    "\n",
    "visualizer.vis_samples(vx_train, 0, 'train')\n",
    "visualizer.vis_samples(vx_val, 0, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    loss_dict = {}\n",
    "\n",
    "    training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader, leave=False):\n",
    "            pos, neg = data\n",
    "            pos = pos.to(device)\n",
    "            neg = neg.to(device)\n",
    "\n",
    "            x = torch.cat((pos, neg), dim=0)\n",
    "\n",
    "            y_pos = torch.tensor([1, 1], dtype=torch.float).repeat(pos.shape[0], 1)\n",
    "            y_neg = torch.tensor([0, 1], dtype=torch.float).repeat(neg.shape[0], 1)\n",
    "            label = torch.cat((y_pos, y_neg), dim=0).to(device)\n",
    "\n",
    "            N = x.shape[0]\n",
    "\n",
    "            y_img, y_mask = model(x)\n",
    "            loss, loss_dict1 = model.loss_fn(x, label, y_img, y_mask)\n",
    "\n",
    "            for k, v in loss_dict1.items():\n",
    "                if k not in loss_dict:\n",
    "                    loss_dict[k] = 0\n",
    "                loss_dict[k] += v * N\n",
    "    \n",
    "    model.train(training)\n",
    "    \n",
    "    for k in loss_dict:\n",
    "        loss_dict[k] /= len(dataloader.dataset)\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = hparams['num_epochs']\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparams['lr'])\n",
    "\n",
    "model.hparams = hparams\n",
    "model.train()\n",
    "step = 0\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for data in tqdm(train_loader, leave=False):\n",
    "        pos, neg = data\n",
    "        pos = pos.to(device)\n",
    "        neg = neg.to(device)\n",
    "\n",
    "        x = torch.cat((pos, neg), dim=0)\n",
    "        \n",
    "        y_pos = torch.tensor([1, 1], dtype=torch.float).repeat(pos.shape[0], 1)\n",
    "        y_neg = torch.tensor([0, 1], dtype=torch.float).repeat(neg.shape[0], 1)\n",
    "        label = torch.cat((y_pos, y_neg), dim=0).to(device)\n",
    "        \n",
    "        y_img, y_mask = model(x)\n",
    "        loss, loss_dict = model.loss_fn(x, label, y_img, y_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        for k, v in loss_dict.items():\n",
    "            writer.add_scalar(f'train/{k}', v.item(), step)\n",
    "    \n",
    "    val_loss_dict = evaluate(model, val_loader)\n",
    "    for k, v in val_loss_dict.items():\n",
    "        writer.add_scalar(f'val/{k}', v, step)\n",
    "    \n",
    "    # visualize\n",
    "    visualizer.vis_samples(vx_train, step, 'train')\n",
    "    visualizer.vis_samples(vx_val, step, 'val')\n",
    "\n",
    "    if val_loss_dict['loss'] < best_val_loss:\n",
    "        best_val_loss = val_loss_dict['loss']\n",
    "        torch.save(model.state_dict(), logdir/'best_model.pth')\n",
    "\n",
    "torch.save(model.state_dict(), logdir/'last_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(logdir/'best_model.pth'))\n",
    "loss_dict = evaluate(model, val_loader)\n",
    "print(loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
